from re import S
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import joblib
import os
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, brier_score_loss
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR
from make_pdf import create_pdf, build_pdf, add_title, add_subtitle, add_paragraph, add_spacer, add_list, add_image, add_table
from scipy.stats import shapiro, kstest, f_oneway, friedmanchisquare, wilcoxon
from statsmodels.stats.multicomp import pairwise_tukeyhsd
import scikit_posthocs as sp

# Lectura de datos
ad_df = pd.read_csv("Datos_abiertos_admision_2021_1_2024_1.csv")

# Limpieza de columnas y eliminar filas con valores nulos
ad_df = ad_df.drop(['IDHASH', 'COLEGIO_DEPA', 'COLEGIO_PROV', 'COLEGIO_DIST',
                    'COLEGIO_PAIS', 'DOMICILIO_DEPA', 'DOMICILIO_PROV',
                    'DOMICILIO_DIST', 'NACIMIENTO_PAIS', 'NACIMIENTO_DEPA',
                    'NACIMIENTO_PROV', 'NACIMIENTO_DIST'], axis=1).dropna()

print("\n" + "="*60)
print("ESTAD√çSTICAS DESCRIPTIVAS")
print("="*60+"\n")
estadisticas = ad_df.describe()
print(estadisticas)

# Categorizaci√≥n de datos
columnas_categoricas = ['COLEGIO', 'ESPECIALIDAD', 'SEXO', 'MODALIDAD', 'INGRESO']
label_encoders = {}
for col in columnas_categoricas:
  le = LabelEncoder()
  col_name = col + "_ENCODED"
  ad_df[col_name] = le.fit_transform(ad_df[col])
  label_encoders[col] = le
  ad_df = ad_df.drop(col, axis=1)

# Visualizaci√≥n de correlaciones
plt.figure(figsize=(10, 8))
sns.heatmap(ad_df.corr(), annot=True, cmap='coolwarm', center=0)
plt.tight_layout()
plt.savefig("images/matriz_correlacion.png")

train_size = 0.8
test_size = 1 - train_size

# Preparaci√≥n de datos
X = ad_df.drop(['INGRESO_ENCODED'], axis=1)
y = ad_df['INGRESO_ENCODED']
X, y = shuffle(X, y, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)

# Estandarizaci√≥n
ss = StandardScaler()
X_train_scaled = ss.fit_transform(X_train)
X_test_scaled = ss.transform(X_test)

print("\n" + "="*60)
print("INFORMACI√ìN DE ENTRENAMIENTO")
print("="*60)

print("\nDatos preparados - Shape del conjunto de entrenamiento:", X_train_scaled.shape)
print("Datos preparados - Shape del conjunto de prueba:", X_test_scaled.shape)

train_length = X_train_scaled.shape[0]
test_length = X_test_scaled.shape[0]

# Diccionario para almacenar modelos y resultados
models = {}
results = {}

print("\n" + "="*60)
print("ENTRENAMIENTO Y EVALUACI√ìN DE MODELOS")
print("="*60)

# 1. Regresi√≥n Lineal
print("\n1. Regresi√≥n Lineal:")
lr = LinearRegression()
lr.fit(X_train_scaled, y_train)
lr_pred = lr.predict(X_test_scaled)

models['Regresi√≥n Lineal'] = lr
results['Regresi√≥n Lineal'] = {
    'y_real': y_test,
    'y_pred': lr_pred,
    'r2': r2_score(y_test, lr_pred),
    'mse': mean_squared_error(y_test, lr_pred),
    'rmse': np.sqrt(mean_squared_error(y_test, lr_pred)),
    'mae': mean_absolute_error(y_test, lr_pred),
    'brier': brier_score_loss(y_test, np.clip(lr_pred, 0, 1))
}

print(f"R¬≤ Score: {results['Regresi√≥n Lineal']['r2']:.4f}")
print(f"MSE: {results['Regresi√≥n Lineal']['mse']:.4f}")
print(f"RMSE: {results['Regresi√≥n Lineal']['rmse']:.4f}")
print(f"MAE: {results['Regresi√≥n Lineal']['mae']:.4f}")
print(f"Brier Score: {results['Regresi√≥n Lineal']['brier']:.4f}")

# 2. Bosques Aleatorios
print("\n2. Bosques Aleatorios:")
rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)  # Bosques Aleatorios no necesita estandarizaci√≥n
rf_pred = rf.predict(X_test)

models['Bosques Aleatorios'] = rf
results['Bosques Aleatorios'] = {
    'y_real': y_test,
    'y_pred': rf_pred,
    'r2': r2_score(y_test, rf_pred),
    'mse': mean_squared_error(y_test, rf_pred),
    'rmse': np.sqrt(mean_squared_error(y_test, rf_pred)),
    'mae': mean_absolute_error(y_test, rf_pred),
    'brier': brier_score_loss(y_test, np.clip(rf_pred, 0, 1))
}

print(f"R¬≤ Score: {results['Bosques Aleatorios']['r2']:.4f}")
print(f"MSE: {results['Bosques Aleatorios']['mse']:.4f}")
print(f"RMSE: {results['Bosques Aleatorios']['rmse']:.4f}")
print(f"MAE: {results['Bosques Aleatorios']['mae']:.4f}")
print(f"Brier Score: {results['Bosques Aleatorios']['brier']:.4f}")

# 3. Regresi√≥n de Vectores de Soporte
print("\n3. Support Vector Regression:")
svr = SVR(kernel='rbf', C=1.0, gamma='scale')
svr.fit(X_train_scaled, y_train)
svr_pred = svr.predict(X_test_scaled)

models['Regresi√≥n de Vectores de Soporte'] = svr
results['Regresi√≥n de Vectores de Soporte'] = {
    'y_real': y_test,
    'y_pred': svr_pred,
    'r2': r2_score(y_test, svr_pred),
    'mse': mean_squared_error(y_test, svr_pred),
    'rmse': np.sqrt(mean_squared_error(y_test, svr_pred)),
    'mae': mean_absolute_error(y_test, svr_pred),
    'brier': brier_score_loss(y_test, np.clip(svr_pred, 0, 1))
}

print(f"R¬≤ Score: {results['Regresi√≥n de Vectores de Soporte']['r2']:.4f}")
print(f"MSE: {results['Regresi√≥n de Vectores de Soporte']['mse']:.4f}")
print(f"RMSE: {results['Regresi√≥n de Vectores de Soporte']['rmse']:.4f}")
print(f"MAE: {results['Regresi√≥n de Vectores de Soporte']['mae']:.4f}")
print(f"Brier Score: {results['Regresi√≥n de Vectores de Soporte']['brier']:.4f}")

# 4. Potenciaci√≥n de Gradiente
print("\n4. Potenciaci√≥n de Gradiente:")
gb = GradientBoostingRegressor(n_estimators=100, random_state=42)
gb.fit(X_train, y_train)
gb_pred = gb.predict(X_test)

models['Potenciaci√≥n de Gradiente'] = gb
results['Potenciaci√≥n de Gradiente'] = {
    'y_real': y_test,
    'y_pred': gb_pred,
    'r2': r2_score(y_test, gb_pred),
    'mse': mean_squared_error(y_test, gb_pred),
    'rmse': np.sqrt(mean_squared_error(y_test, gb_pred)),
    'mae': mean_absolute_error(y_test, gb_pred),
    'brier': brier_score_loss(y_test, np.clip(gb_pred, 0, 1))
}

print(f"R¬≤ Score: {results['Potenciaci√≥n de Gradiente']['r2']:.4f}")
print(f"MSE: {results['Potenciaci√≥n de Gradiente']['mse']:.4f}")
print(f"RMSE: {results['Potenciaci√≥n de Gradiente']['rmse']:.4f}")
print(f"MAE: {results['Potenciaci√≥n de Gradiente']['mae']:.4f}")
print(f"Brier Score: {results['Potenciaci√≥n de Gradiente']['brier']:.4f}")

# TABLA COMPARATIVA DE RESULTADOS
print("\n" + "="*60)
print("COMPARACI√ìN DE RESULTADOS")
print("="*60)

comparison_df = pd.DataFrame({
    'Modelo': list(results.keys()),
    'R¬≤ Score': [results[model]['r2'] for model in results.keys()],
    'MSE': [results[model]['mse'] for model in results.keys()],
    'RMSE': [results[model]['rmse'] for model in results.keys()],
    'MAE': [results[model]['mae'] for model in results.keys()],
    'Brier': [results[model]['brier'] for model in results.keys()]
})

# Brier Score Comparison Plot
plt.figure(figsize=(5, 6))
brier_scores = comparison_df['Brier']
bars_brier = plt.bar(comparison_df['Modelo'], brier_scores, color=['skyblue', 'lightgreen', 'lightcoral', 'lightyellow'])
plt.title('Comparaci√≥n Brier Score (menor es mejor)')
plt.ylabel('Brier Score')
plt.xticks(rotation=45)
plt.grid(axis='y', alpha=0.3)
for bar, value in zip(bars_brier, brier_scores):
    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + (max(brier_scores) * 0.01), f'{value:.4f}', ha='center', va='bottom', fontsize=10)
plt.tight_layout()
plt.savefig("images/comparacion_brier.png")
#plt.show()

comparison_df = comparison_df.sort_values('R¬≤ Score', ascending=False).round(4)
print(comparison_df)

# VISUALIZACIONES COMPARATIVAS

# 1. Gr√°fico de barras para R¬≤ Score
plt.figure(figsize=(5, 6))
r2_scores = comparison_df['R¬≤ Score'] * 100
bars_r2 = plt.bar(comparison_df['Modelo'], r2_scores, color=['skyblue', 'lightgreen', 'lightcoral', 'lightyellow'])
plt.title('Comparaci√≥n R¬≤ Score')
plt.ylabel('R¬≤ Score (%)')
plt.xticks(rotation=45)
plt.grid(axis='y', alpha=0.3)
for bar, value in zip(bars_r2, r2_scores):
    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 1, f'{value:.1f}%', ha='center', va='bottom', fontsize=10)
plt.tight_layout()
plt.savefig("images/comparacion_r2.png")

# 2. Gr√°fico de barras para MSE
plt.figure(figsize=(5, 6))
mse_vals = comparison_df['MSE']
bars_mse = plt.bar(comparison_df['Modelo'], mse_vals, color=['skyblue', 'lightgreen', 'lightcoral', 'lightyellow'])
plt.title('Comparaci√≥n MSE (menor es mejor)')
plt.ylabel('ECP')
plt.xticks(rotation=45)
plt.grid(axis='y', alpha=0.3)
for bar, value in zip(bars_mse, mse_vals):
    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + (max(mse_vals) * 0.01), f'{value:.3f}', ha='center', va='bottom', fontsize=10)
plt.tight_layout()
plt.savefig("images/comparacion_mse.png")

# Remove any code that uses confusion_matrix or ConfusionMatrixDisplay

clasificacion_df = pd.DataFrame({
    'Modelo': list(results.keys()),
    'R2': [results[model]['r2'] for model in results.keys()],
    'EAP': [results[model]['mae'] for model in results.keys()],
    'RECP': [results[model]['rmse'] for model in results.keys()],
    'ECP': [results[model]['mse'] for model in results.keys()],
    'Brier': [results[model]['brier'] for model in results.keys()],
})

clasificacion_df = clasificacion_df.sort_values('R2', ascending=False).round(4)
print(clasificacion_df)

# 3. Gr√°fico de barras para Precisi√≥n (en porcentaje)
plt.figure(figsize=(5, 6))
precisiones = clasificacion_df['R2'] * 100
bars = plt.bar(clasificacion_df['Modelo'], precisiones, color=['skyblue', 'lightgreen', 'lightcoral', 'lightyellow'])
plt.title('Comparaci√≥n de Precisi√≥n')
plt.ylabel('Precisi√≥n (%)')
plt.ylim(0, 100)
plt.xticks(rotation=45)
plt.grid(axis='y', alpha=0.3)
for bar, value in zip(bars, precisiones):
    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 1, f'{value:.1f}%', ha='center', va='bottom', fontsize=10)
plt.tight_layout()
plt.savefig("images/comparacion_precision.png")

# AN√ÅLISIS DE IMPORTANCIA DE CARACTER√çSTICAS (para modelos que lo soportan)
print("\n" + "="*60)
print("IMPORTANCIA DE CARACTER√çSTICAS")
print("="*60)

# Random Forest - Importancia de caracter√≠sticas
print("\nRandom Forest - Importancia de caracter√≠sticas:")
rf_importance = pd.DataFrame({
    'Feature': X.columns,
    'Importance': rf.feature_importances_
}).sort_values('Importance', ascending=False)
print(rf_importance)

# Gradient Boosting - Importancia de caracter√≠sticas
print("\nGradient Boosting - Importancia de caracter√≠sticas:")
gb_importance = pd.DataFrame({
    'Feature': X.columns,
    'Importance': gb.feature_importances_
}).sort_values('Importance', ascending=False)
print(gb_importance)

# Regresi√≥n Lineal - Coeficientes
print("\nRegresi√≥n Lineal - Coeficientes:")
lr_coef = pd.DataFrame({
    'Feature': X.columns,
    'Coefficient': lr.coef_
}).sort_values('Coefficient', key=abs, ascending=False)
print(lr_coef)

# Visualizaci√≥n de importancia de caracter√≠sticas
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
plt.barh(rf_importance['Feature'], rf_importance['Importance'])
plt.title('Random Forest - Importancia')
plt.xlabel('Importancia')

plt.subplot(1, 3, 2)
plt.barh(gb_importance['Feature'], gb_importance['Importance'])
plt.title('Gradient Boosting - Importancia')
plt.xlabel('Importancia')

plt.subplot(1, 3, 3)
plt.barh(lr_coef['Feature'], lr_coef['Coefficient'])
plt.title('Regresi√≥n Lineal - Coeficientes')
plt.xlabel('Coeficiente')
plt.tight_layout()

# NORMALITY TESTS (Shapiro-Wilk and Kolmogorov-Smirnov)
print("\n" + "="*60)
print("TESTS DE NORMALIDAD DE RESIDUALES")
print("="*60)

normality_tests = []
for model_name in results.keys():
    residuals = y_test - results[model_name]['y_pred']
    if len(residuals) <= 50:
        stat, p = shapiro(residuals)
        test = 'Shapiro-Wilk'
    else:
        stat, p = kstest(residuals, 'norm', args=(np.mean(residuals), np.std(residuals)))
        test = 'Kolmogorov-Smirnov'
        
    normality_tests.append({
        'model_name': model_name,
        'stat': stat,
        'p': p,
        'test': test
    })
    print(f"\nModelo: {model_name}")
    print(f"  {test}: stat={stat}, p={p}")

normality_df = pd.DataFrame(normality_tests)

# Determinar si los modelos pasan normalidad (p > 0.05)
all_normal = all(normality_df['p'] > 0.05)

# Prueba ANOVA o Friedman
print("\n" + "="*60)
print("COMPARACI√ìN DE MODELOS: ANOVA o FRIEDMAN")
print("="*60)

residuals_list = [results[m]['y_pred'] - results[m]['y_real'] for m in results.keys()]
residuals_matrix = np.column_stack(residuals_list)
model_names = list(results.keys())

posthoc_result = None
posthoc_plot_path = None

if all_normal:
    # ANOVA
    anova_stat, anova_p = f_oneway(*residuals_list)
    print(f"\nANOVA: stat={anova_stat}, p={anova_p}")
    test_used = 'ANOVA'
    test_stat, test_p = anova_stat, anova_p
    # Post-hoc: Tukey HSD
    stacked_residuals = np.concatenate(residuals_list)
    group_labels = np.concatenate([[name]*len(residuals_list[0]) for name in model_names])
    tukey = pairwise_tukeyhsd(stacked_residuals, group_labels)
    posthoc_result = pd.DataFrame(data=tukey._results_table.data[1:], columns=tukey._results_table.data[0])
    # Plot
    plt.figure(figsize=(15,9))
    sp.sign_plot(posthoc_result['reject'].astype(float).values.reshape(len(model_names),-1), model_names)
    plt.title('Tukey HSD Post-hoc')
    posthoc_plot_path = 'images/posthoc_tukey.png'
    plt.savefig(posthoc_plot_path)
    plt.close()
else:
    # Friedman test
    friedman_stat, friedman_p = friedmanchisquare(*[residuals_matrix[:,i] for i in range(residuals_matrix.shape[1])])
    print(f"\nFriedman: stat={friedman_stat}, p={friedman_p}")
    test_used = 'Friedman'
    test_stat, test_p = friedman_stat, friedman_p

    if friedman_p < 0.05:
        print("\n" + "="*60)
        print("AN√ÅLISIS POST-HOC: PRUEBA DE NEMENYI")
        print("="*60)
        
        # Aplicar prueba de Nemenyi
        nemenyi_result = sp.posthoc_nemenyi_friedman(residuals_matrix)
        posthoc_result = nemenyi_result
        
        # Mostrar matriz de p-values
        print("\nMatriz de p-values de Nemenyi:")
        print(nemenyi_result.round(4))
        
        # Interpretaci√≥n de resultados
        print("\n" + "="*60)
        print("INTERPRETACI√ìN DE RESULTADOS")
        print("="*60)
        
        alpha = 0.05
        significant_pairs = []
        
        print(f"\nComparaciones significativas (p < {alpha}):")
        for i in range(len(model_names)):
            for j in range(i+1, len(model_names)):
                p_value = nemenyi_result.iloc[i, j]
                if p_value < alpha:
                    significant_pairs.append((model_names[i], model_names[j], p_value))
                    print(f"  {model_names[i]} vs {model_names[j]}: p = {p_value:.4f} *")
        
        if not significant_pairs:
            print("  No se encontraron diferencias significativas entre pares de modelos")
        
        # Ranking de modelos basado en rendimiento promedio
        print(f"\nRanking de modelos (basado en R¬≤ Score):")
        ranking_df = comparison_df.copy()
        ranking_df['Ranking'] = range(1, len(ranking_df) + 1)
        
        for idx, row in ranking_df.iterrows():
            print(f"  {row['Ranking']}. {row['Modelo']} (R¬≤ = {row['R¬≤ Score']:.4f})")
        
        # Grupos homog√©neos (modelos sin diferencias significativas)
        print(f"\nGrupos homog√©neos (sin diferencias significativas):")
        
        # Crear grupos basados en las comparaciones no significativas
        groups = []
        models_processed = set()
        
        for i, model1 in enumerate(model_names):
            if model1 not in models_processed:
                current_group = [model1]
                models_processed.add(model1)
                
                for j, model2 in enumerate(model_names):
                    if i != j and model2 not in models_processed:
                        p_value = nemenyi_result.iloc[i, j]
                        if p_value >= alpha:
                            current_group.append(model2)
                            models_processed.add(model2)
                
                if len(current_group) > 1:
                    groups.append(current_group)
        
        if groups:
            for i, group in enumerate(groups, 1):
                print(f"  Grupo {i}: {', '.join(group)}")
        else:
            print("  Todos los modelos son significativamente diferentes")
        
        # Crear gr√°fico mejorado
        plt.figure(figsize=(12, 8))
        
        # Heatmap de p-values
        plt.subplot(2, 1, 1)
        sns.heatmap(nemenyi_result, annot=True, fmt='.4f', cmap='RdYlBu_r', 
                    xticklabels=model_names, yticklabels=model_names,
                    cbar_kws={'label': 'p-value'})
        plt.title('Prueba de Nemenyi - Matriz de p-values')
        
        # Gr√°fico de significancia
        plt.subplot(2, 1, 2)
        significance_matrix = (nemenyi_result < alpha).astype(int)
        sns.heatmap(significance_matrix, annot=True, fmt='d', cmap='RdYlGn_r',
                    xticklabels=model_names, yticklabels=model_names,
                    cbar_kws={'label': 'Significativo (1) / No significativo (0)'})
        plt.title(f'Diferencias significativas (Œ± = {alpha})')
        
        plt.tight_layout()
        posthoc_plot_path = 'images/posthoc_nemenyi_detailed.png'
        plt.savefig(posthoc_plot_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        # Gr√°fico de rangos promedio (Critical Difference plot)
        plt.figure(figsize=(10, 6))
        
        # Calcular rangos promedio para cada modelo
        avg_ranks = []
        for i in range(len(model_names)):
            # Simular ranking basado en R¬≤ (mejor modelo = menor rango)
            r2_scores = [results[model]['r2'] for model in model_names]
            sorted_indices = np.argsort(r2_scores)[::-1]  # Descendente
            ranks = np.empty_like(sorted_indices)
            ranks[sorted_indices] = np.arange(1, len(sorted_indices) + 1)
            avg_ranks.append(ranks[i])
        
        # Crear el gr√°fico
        y_pos = np.arange(len(model_names))
        bars = plt.barh(y_pos, avg_ranks, color=['skyblue', 'lightgreen', 'lightcoral', 'lightyellow'])
        
        plt.yticks(y_pos, model_names)
        plt.xlabel('Rango Promedio (menor es mejor)')
        plt.title('Rangos Promedio de Modelos - Prueba de Nemenyi')
        plt.grid(axis='x', alpha=0.3)
        
        # A√±adir valores en las barras
        for i, (bar, rank) in enumerate(zip(bars, avg_ranks)):
            plt.text(bar.get_width() + 0.1, bar.get_y() + bar.get_height()/2, 
                    f'{rank:.1f}', va='center', fontweight='bold')
        
        plt.tight_layout()
        plt.savefig('images/nemenyi_ranks.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"\nGr√°ficos guardados:")
        print(f"  - Matriz detallada: {posthoc_plot_path}")
        print(f"  - Rangos promedio: images/nemenyi_ranks.png")
        
    else:
        print("\nNo se realiz√≥ an√°lisis post-hoc porque el test de Friedman no fue significativo")
        posthoc_result = None
        posthoc_plot_path = None
    """# Friedman
    friedman_stat, friedman_p = friedmanchisquare(*[residuals_matrix[:,i] for i in range(residuals_matrix.shape[1])])
    print(f"\nFriedman: stat={friedman_stat}, p={friedman_p}")
    test_used = 'Friedman'
    test_stat, test_p = friedman_stat, friedman_p
    # Post-hoc: Nemenyi
    nemenyi = sp.posthoc_nemenyi_friedman(residuals_matrix)
    posthoc_result = nemenyi
    # Plot
    plt.figure(figsize=(15,9))
    sp.sign_plot(nemenyi.values, model_names)
    plt.title('Nemenyi Post-hoc')
    posthoc_plot_path = 'images/posthoc_nemenyi.png'
    plt.savefig(posthoc_plot_path)
    plt.close()"""

# Residuales
plt.figure(figsize=(10,8))
plt.boxplot(residuals_list, tick_labels=model_names)
plt.title('Boxplot de residuales por modelo')
plt.ylabel('Residuales')
plt.tight_layout()
boxplot_path = 'images/boxplot_residuales.png'
plt.savefig(boxplot_path)
plt.close()

# MEJOR MODELO
best_model_name = comparison_df.iloc[0]['Modelo']
best_r2 = comparison_df.iloc[0]['R¬≤ Score']

print(f"\nüèÜ MEJOR MODELO: {best_model_name}")
print(f"R¬≤ Score: {best_r2:.4f}")
print(f"Esto significa que el modelo explica el {best_r2*100:.2f}% de la varianza en los datos.")

# GUARDAR MODELOS Y SCALERS
print("\n" + "="*60)
print("GUARDANDO MODELOS Y SCALERS")
print("="*60+"\n")

# Crear directorio para modelos si no existe
models_dir = "models"
if not os.path.exists(models_dir):
    os.makedirs(models_dir)
    print(f"Directorio '{models_dir}' creado.")

# Guardar todos los modelos entrenados
for model_name, model in models.items():
    model_filename = f"{models_dir}/{model_name.replace(' ', '_').lower()}.pkl"
    with open(model_filename, 'wb') as f:
        joblib.dump(model, f)
    print(f"Modelo '{model_name}' guardado en: {model_filename}")

# Guardar el StandardScaler
scaler_filename = f"{models_dir}/standard_scaler.pkl"
with open(scaler_filename, 'wb') as f:
    joblib.dump(ss, f)
print(f"StandardScaler guardado en: {scaler_filename}")

# Guardar los LabelEncoders
encoders_filename = f"{models_dir}/label_encoders.pkl"
with open(encoders_filename, 'wb') as f:
    joblib.dump(label_encoders, f)
print(f"LabelEncoders guardados en: {encoders_filename}")

# Guardar informaci√≥n de las columnas para preprocesamiento futuro
column_info = {
    'feature_columns': list(X.columns),
    'categorical_columns': columnas_categoricas,
    'target_column': 'INGRESO_ENCODED'
}

column_info_filename = f"{models_dir}/column_info.pkl"
with open(column_info_filename, 'wb') as f:
    joblib.dump(column_info, f)
print(f"Informaci√≥n de columnas guardada en: {column_info_filename}")

# Guardar el mejor modelo por separado para f√°cil acceso
best_model = models[best_model_name]
best_model_filename = f"{models_dir}/best_model.pkl"
with open(best_model_filename, 'wb') as f:
    joblib.dump(best_model, f)
print(f"Mejor modelo '{best_model_name}' guardado en: {best_model_filename}")

print(f"\n‚úÖ Todos los modelos y scalers han sido guardados en el directorio '{models_dir}/'")
print("Archivos guardados:")
print(f"  - Modelos individuales: {len(models)} archivos .pkl")
print(f"  - StandardScaler: standard_scaler.pkl")
print(f"  - LabelEncoders: label_encoders.pkl")
print(f"  - Informaci√≥n de columnas: column_info.pkl")
print(f"  - Mejor modelo: best_model.pkl")

doc, story = create_pdf("reporte")

# Titulo
add_title(story, "Reporte de Modelos de Clasificaci√≥n")
add_spacer(story, 1,12)

# Dataset
#ad_df_short = ad_df.head()
#add_subtitle(story, "Dataset")
#add_table(story, ad_df_short)
#add_spacer(story, 1,12)

# Modelos usados
models_list = ['Regresi√≥n Lineal',
          'Bosques Aleatorios',
          'Regresi√≥n de Vectores de Soporte',
          'Potenciaci√≥n de Gradiente']
add_subtitle(story, "Modelos a comparar")
add_list(story, models_list)
add_spacer(story, 1,6)

# Matriz de correlacion
add_subtitle(story, "Matriz de correlaci√≥n")
add_image(story, "images/matriz_correlacion.png", 500 , 400)
add_spacer(story, 1,6)

# Datos de entrenamiento y prueba
add_subtitle(story, "Datos de entrenamiento y prueba")
add_paragraph(story, f"Tama√±o de conjunto de datos de entrenamiento: {train_length} ({train_size*100:.2f}%)")
add_paragraph(story, f"Tama√±o de conjunto de datos de prueba: {test_length} ({test_size*100:.2f}%)")
add_spacer(story, 1,6)

# Comparaci√≥n de modelos
add_subtitle(story, "Comparaci√≥n R2")
add_image(story, "images/comparacion_r2.png", 240, 300)
add_subtitle(story, "Comparaci√≥n Error Cuadrado Promedio")
add_image(story, "images/comparacion_mse.png", 240, 300)
add_subtitle(story, "Comparaci√≥n Precisi√≥n")
add_image(story, "images/comparacion_precision.png", 240, 300)
add_subtitle(story, "Comparaci√≥n Brier Score")
add_image(story, "images/comparacion_brier.png", 240, 300)

# An√°lisis de clasificaci√≥n
add_subtitle(story, "An√°lisis de clasificaci√≥n")
add_table(story, clasificacion_df)
add_spacer(story, 1,6)

add_subtitle(story, "Test de normalidad de residuales (Shapiro-Wilk y Kolmogorov-Smirnov)")
normality_table = pd.DataFrame(normality_tests)
add_table(story, normality_table)
add_spacer(story, 1,6)

add_subtitle(story, "Comparaci√≥n de modelos (ANOVA o Friedman)")
add_paragraph(story, f"Test usado: {test_used}")
add_paragraph(story, f"stat={test_stat}, p={test_p}")
if test_p < 0.05:
    add_paragraph(story, "\nDiferencias significativas entre modelos (p < 0.05)")
else:
    add_paragraph(story, "\nNo hay diferencias significativas entre modelos (p >= 0.05)")
add_spacer(story, 1,6)

if test_p < 0.05:
    add_subtitle(story, "An√°lisis Post-hoc: Prueba de Nemenyi")
    add_paragraph(story, f"Dado que el test de Friedman fue significativo (p = {test_p:.2e}), se procedi√≥ con el an√°lisis post-hoc usando la prueba de Nemenyi.")
    add_spacer(story, 1,6)
    
    # Agregar matriz de p-values si existe
    if posthoc_result is not None:
        add_subtitle(story, "Matriz de p-values - Nemenyi")
        add_image(story, "images/posthoc_nemenyi_detailed.png", 400, 300)
        add_spacer(story, 1,6)
        
        add_subtitle(story, "Rangos promedio de modelos")
        add_image(story, "images/nemenyi_ranks.png", 400, 250)
        add_spacer(story, 1,6)
        
        # Interpretar resultados
        alpha = 0.05
        significant_pairs = []
        
        for i in range(len(model_names)):
            for j in range(i+1, len(model_names)):
                p_value = posthoc_result.iloc[i, j]
                if p_value < alpha:
                    significant_pairs.append((model_names[i], model_names[j], p_value))
        
        if significant_pairs:
            add_subtitle(story, "Comparaciones significativas")
            for pair in significant_pairs:
                add_paragraph(story, f"\n\t‚Ä¢ {pair[0]} vs {pair[1]}: p = {pair[2]:.2e}")
        else:
            add_paragraph(story, f"No se encontraron diferencias significativas entre pares de modelos (Œ± = {alpha})")
        
        add_spacer(story, 1,6)
        
        add_subtitle(story, "Conclusiones del an√°lisis post-hoc")
        add_paragraph(story, "La prueba de Nemenyi revel√≥ las siguientes conclusiones:")
        
        add_paragraph(story, f"1. El test de Friedman confirm√≥ diferencias significativas entre los modelos (p = {test_p:.2e})")
        add_paragraph(story, f"2. El modelo '{best_model_name}' mostr√≥ el mejor rendimiento con R¬≤ = {best_r2:.4f}")
        add_paragraph(story, "3. Los an√°lisis post-hoc identificaron qu√© modelos difieren significativamente entre s√≠")
        add_paragraph(story, "4. Esta informaci√≥n es crucial para la selecci√≥n del modelo √≥ptimo")
else:
    add_paragraph(story, "No se realiz√≥ an√°lisis post-hoc porque el test de Friedman no fue significativo.")

# Mejor modelo
add_subtitle(story, "Modelo Optimo")
add_paragraph(story, f"<b>MEJOR MODELO:</b> {best_model_name}")
add_paragraph(story, f"‚Ä¢ R2: {results[best_model_name]['r2']}")
add_paragraph(story, f"‚Ä¢ Error Absoluto Promedio: {results[best_model_name]['mae']}")
add_paragraph(story, f"‚Ä¢ Raiz del Error Cuadrado Promedio: {results[best_model_name]['rmse']}")
add_paragraph(story, f"‚Ä¢ Error Cuadrado Promedio: {results[best_model_name]['mse']}")
add_paragraph(story, f"‚Ä¢ Brier Score: {results[best_model_name]['brier']}")

add_subtitle(story, "Gr√°fico de residuales por modelo")
add_image(story, boxplot_path, 400, 300)
add_spacer(story, 1,6)

# Generar PDF
build_pdf(doc, story)
print("\n‚úÖ PDF generado correctamente")

def run_streamlit():
    os.system('streamlit run app.py --server.port 8501 --server.headless true -server.fileWatcherType none --browser.gatherUsageStats false')